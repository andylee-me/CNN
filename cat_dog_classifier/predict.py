import os
import shutil
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# 1ï¸âƒ£ æ¨¡å‹è·¯å¾‘
model_path = "model/catdog_model.h5"
assert os.path.exists(model_path), f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹ {model_path}"
print(f"âœ… è¼‰å…¥æ¨¡å‹: {model_path}")
model = load_model(model_path)

# 2ï¸âƒ£ è³‡æ–™å¤¾
train_dir = "file/kaggle_cats_vs_dogs_f/kaggle_cats_vs_dogs_f/train"
val_dir = "file/kaggle_cats_vs_dogs_f/kaggle_cats_vs_dogs_f/val"
img_size = (128, 128)

# 3ï¸âƒ£ é è™•ç†å™¨
datagen = ImageDataGenerator(rescale=1./255)
datasets = {"train": train_dir, "val": val_dir}

# 4ï¸âƒ£ æ¯æ¬¡å…ˆåˆªæ‰èˆŠçš„ misclassified
misclassified_dir = "misclassified"
if os.path.exists(misclassified_dir):
    shutil.rmtree(misclassified_dir)
os.makedirs(misclassified_dir, exist_ok=True)

def ensure_subdirs(base_path):
    """ç¢ºä¿ cat/dog å­è³‡æ–™å¤¾å­˜åœ¨"""
    for cls in ["cat", "dog"]:
        os.makedirs(os.path.join(base_path, cls), exist_ok=True)

def evaluate_and_save(dataset_name, dataset_path):
    print(f"\nğŸš€ é–‹å§‹æª¢æŸ¥ {dataset_name} è³‡æ–™é›†...")

    gen = datagen.flow_from_directory(
        dataset_path,
        target_size=img_size,
        batch_size=1,
        class_mode='binary',
        shuffle=False
    )

    total_images = len(gen.filepaths)
    print(f"â¡ å…±æ‰¾åˆ° {total_images} å¼µåœ–ç‰‡")

    pred_probs = model.predict(gen, verbose=1)
    pred_labels = (pred_probs > 0.5).astype(int).flatten()
    true_labels = gen.classes
    file_paths = gen.filepaths

    # å»ºç«‹ dataset å°æ‡‰è³‡æ–™å¤¾
    dataset_mis_dir = os.path.join(misclassified_dir, dataset_name)
    os.makedirs(dataset_mis_dir, exist_ok=True)
    ensure_subdirs(dataset_mis_dir)

    misclassified_count = 0
    debug_samples = 0

    for idx, (true_label, pred_label) in enumerate(zip(true_labels, pred_labels)):
        if true_label != pred_label:
            src_path = file_paths[idx]
            filename = os.path.basename(src_path)

            if not os.path.exists(src_path):
                print(f"âš ï¸ æ‰¾ä¸åˆ°åŸå§‹æª”æ¡ˆ: {src_path}")
                continue

            dst_cls = "cat" if true_label == 0 else "dog"
            dst_path = os.path.join(dataset_mis_dir, dst_cls, f"wrong_pred_{filename}")
            shutil.copy(src_path, dst_path)
            misclassified_count += 1

            if debug_samples < 5:
                print(f"âŒ Misclassified -> {src_path} â†’ {dst_path}")
                debug_samples += 1

    acc = (1 - misclassified_count / total_images) * 100
    print(f"âœ… {dataset_name} é›†: å…± {total_images} å¼µ, éŒ¯èª¤ {misclassified_count}, æº–ç¢ºç‡ {acc:.2f}%")

    # å¦‚æœå®Œå…¨æ²’æœ‰éŒ¯èª¤ï¼Œè‡³å°‘æ”¾å€‹ README.txt ä»¥å… zip ç©ºè³‡æ–™å¤¾
    if misclassified_count == 0:
        with open(os.path.join(dataset_mis_dir, "README.txt"), "w") as f:
            f.write("This dataset has no misclassified images ğŸ‰")

# âœ… è·‘ train + val
for name, path in datasets.items():
    evaluate_and_save(name, path)

# âœ… æœ€å¾Œå°å‡ºå®Œæ•´ç›®éŒ„æ¨¹
print("\nğŸ“‚ æœ€çµ‚ misclassified è³‡æ–™å¤¾çµæ§‹ï¼š")
for root, dirs, files in os.walk(misclassified_dir):
    level = root.replace(misclassified_dir, "").count(os.sep)
    indent = " " * 2 * level
    print(f"{indent}ğŸ“ {os.path.basename(root)}/")
    for f in files:
        print(f"{indent}  â”œâ”€â”€ {f}")
