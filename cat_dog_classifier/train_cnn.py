import os
import json
import shutil
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.imagenet_utils import preprocess_input

# =========================
# ğŸ”§ åƒæ•¸è¨­å®š
# =========================
# æ˜¯å¦ä½¿ç”¨é è¨“ç·´æ¬Šé‡ï¼ˆå»ºè­° Trueï¼‰
USE_PRETRAINED = False             # True: ImageNet é è¨“ç·´ï¼›False: å¾é ­è¨“ç·´
UNFREEZE_LAST_BLOCK = True        # True: å¾®èª¿ VGG16 çš„æœ€å¾Œä¸€å€‹ conv_blockï¼ˆæ›´é«˜æº–ç¢ºç‡ï¼‰
img_size = (224, 224)             # VGG çš„æ¨™æº–è¼¸å…¥å°ºå¯¸
batch_size = 32
train_dir = "file/kaggle_cats_vs_dogs_f/train"
val_dir   = "file/kaggle_cats_vs_dogs_f/val"
model_dir = "model"
os.makedirs(model_dir, exist_ok=True)
model_path = os.path.join(model_dir, "vgg16_catdog.h5")
class_indices_path = os.path.join(model_dir, "class_indices.json")

# =========================
# ğŸ” è³‡æ–™å‰è™•ç†ï¼ˆVGG = 'caffe' æ¨¡å¼ï¼‰
# - ä¸è¦å† rescale=1./255
# - ä½¿ç”¨ preprocess_input(..., mode='caffe')
# =========================
train_datagen = ImageDataGenerator(
    preprocessing_function=lambda x: preprocess_input(x, mode="caffe"),
    rotation_range=8,
    width_shift_range=0.03,
    height_shift_range=0.03,
    zoom_range=0.08,
    horizontal_flip=True,
    fill_mode='reflect'
)

val_datagen = ImageDataGenerator(
    preprocessing_function=lambda x: preprocess_input(x, mode="caffe")
)

train_gen = train_datagen.flow_from_directory(
    train_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='binary',
    shuffle=True
)

val_gen = val_datagen.flow_from_directory(
    val_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='binary',
    shuffle=False
)

# å„²å­˜ class_indices ä¾›é æ¸¬ä½¿ç”¨
with open(class_indices_path, "w") as f:
    json.dump(train_gen.class_indices, f, indent=2, ensure_ascii=False)
print("ğŸ—‚ï¸ Saved class_indices:", train_gen.class_indices)

# =========================
# ğŸ§± æ¨¡å‹ï¼šVGG16 backbone + è‡ªè¨‚åˆ†é¡é ­
# =========================
weights = "imagenet" if USE_PRETRAINED else None
base = VGG16(include_top=False, weights=weights, input_shape=(img_size[0], img_size[1], 3))

# é è¨­å‡çµå…¨éƒ¨
for layer in base.layers:
    layer.trainable = False

# å¯é¸ï¼šè§£å‡æœ€å¾Œä¸€å€‹ blockï¼ˆblock5_*ï¼‰
if USE_PRETRAINED and UNFREEZE_LAST_BLOCK:
    for layer in base.layers:
        if layer.name.startswith("block5"):
            layer.trainable = True

x = base.output
# VGG16 åŸç”Ÿæ˜¯ Flatten + FCï¼Œé€™è£¡ç”¨ GAP + å°é ­ï¼Œè¨“ç·´æ›´ç©©å®š
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation="relu")(x)
x = Dropout(0.3)(x)
out = Dense(1, activation="sigmoid")(x)  # äºŒåˆ†é¡

model = Model(inputs=base.input, outputs=out)

# =========================
# âš™ ç·¨è­¯
# - å¦‚æœå¾é ­è¨“ç·´ï¼Œå»ºè­°æŠŠ lr èª¿ä½ä¸€é»ä¸¦å»¶é•· epoch
# =========================
init_lr = 1e-4 if USE_PRETRAINED else 5e-4
model.compile(optimizer=Adam(learning_rate=init_lr),
              loss="binary_crossentropy",
              metrics=["accuracy"])

model.summary()

# =========================
# â±ï¸ Callbacks
# =========================
callbacks = [
    ReduceLROnPlateau(monitor="val_loss", factor=0.1, patience=5, verbose=1, min_lr=1e-6),
    EarlyStopping(monitor="val_loss", patience=12, restore_best_weights=True, verbose=1),
    ModelCheckpoint(model_path, monitor="val_accuracy", save_best_only=True, verbose=1)
]

# =========================
# ğŸš€ è¨“ç·´
# =========================
epochs = 60 if USE_PRETRAINED else 100
history = model.fit(
    train_gen,
    epochs=epochs,
    validation_data=val_gen,
    callbacks=callbacks,
    workers=4,
    use_multiprocessing=True
)

# å¦å­˜ä¸€ä»½æœ€çµ‚
model.save(os.path.join(model_dir, "vgg16_catdog_final.h5"))
print("âœ… Saved:", os.path.join(model_dir, "vgg16_catdog_final.h5"))

# =========================
# ğŸ” è©•ä¼°ï¼†è¼¸å‡º misclassified å½±åƒ
# =========================
print("\n================  è©•ä¼°èˆ‡éŒ¯èª¤æ¨£æœ¬åŒ¯å‡º  ================")

# é‡æ–°è¼‰å…¥æœ€ä½³æ¨¡å‹
assert os.path.exists(model_path), f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹ {model_path}"
print(f"âœ… è¼‰å…¥æœ€ä½³æ¨¡å‹: {model_path}")
best_model = load_model(model_path)

# èˆ‡è¨“ç·´ä¸€è‡´çš„å‰è™•ç†
eval_datagen = ImageDataGenerator(
    preprocessing_function=lambda x: preprocess_input(x, mode="caffe")
)

datasets = {"train": train_dir, "val": val_dir}
misclassified_root = "misclassified_vgg16"

# å…ˆæ¸…ç©ºèˆŠçš„
if os.path.exists(misclassified_root):
    shutil.rmtree(misclassified_root)
os.makedirs(misclassified_root, exist_ok=True)

# index->name å°æ‡‰ï¼ˆç”¨å‰›æ‰å­˜çš„ class_indicesï¼‰
with open(class_indices_path, "r") as f:
    class_indices = json.load(f)
idx_to_name = {v: k for k, v in class_indices.items()}

def ensure_subdirs(base_path, class_names=("cat","dog")):
    for cls in class_names:
        os.makedirs(os.path.join(base_path, cls), exist_ok=True)

def evaluate_and_save(dataset_name, dataset_path):
    print(f"\nğŸš€ é–‹å§‹æª¢æŸ¥ {dataset_name} è³‡æ–™é›†...")

    gen = eval_datagen.flow_from_directory(
        dataset_path,
        target_size=img_size,
        batch_size=1,
        class_mode='binary',
        shuffle=False
    )

    total_images = len(gen.filepaths)
    print(f"â¡ å…±æ‰¾åˆ° {total_images} å¼µåœ–ç‰‡")

    pred_probs = best_model.predict(gen, verbose=1)
    pred_labels = (pred_probs > 0.5).astype(int).flatten()
    true_labels = gen.classes
    file_paths  = gen.filepaths

    dataset_mis_dir = os.path.join(misclassified_root, dataset_name)
    os.makedirs(dataset_mis_dir, exist_ok=True)
    ensure_subdirs(dataset_mis_dir, class_names=list(idx_to_name.values()))

    misclassified_count = 0
    debug_samples = 0

    for idx, (t, p) in enumerate(zip(true_labels, pred_labels)):
        if t != p:
            src_path = file_paths[idx]
            filename = os.path.basename(src_path)

            if not os.path.exists(src_path):
                print(f"âš ï¸ æ‰¾ä¸åˆ°åŸå§‹æª”æ¡ˆ: {src_path}")
                continue

            true_name = idx_to_name.get(int(t), str(t))
            dst_path = os.path.join(dataset_mis_dir, true_name, f"wrong_pred_{filename}")
            shutil.copy(src_path, dst_path)
            misclassified_count += 1

            if debug_samples < 5:
                print(f"âŒ Misclassified -> {src_path} â†’ {dst_path}")
                debug_samples += 1

    acc = (1 - misclassified_count / total_images) * 100 if total_images > 0 else 0.0
    print(f"âœ… {dataset_name} é›†: å…± {total_images} å¼µ, éŒ¯èª¤ {misclassified_count}, æº–ç¢ºç‡ {acc:.2f}%")

for name, path in datasets.items():
    evaluate_and_save(name, path)

print("\nğŸ“‚ æœ€çµ‚ misclassified_vgg16 è³‡æ–™å¤¾çµæ§‹ï¼š")
for root, dirs, files in os.walk(misclassified_root):
    level = root.replace(misclassified_root, "").count(os.sep)
    indent = " " * 2 * level
    print(f"{indent}ğŸ“ {os.path.basename(root)}/")
    for f in files:
        print(f"{indent}  â”œâ”€â”€ {f}")
