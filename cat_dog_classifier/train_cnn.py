import os, json, shutil, tensorflow as tf
from tensorflow.keras import mixed_precision
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.imagenet_utils import preprocess_input
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import AdamW

# ============== åŸºæœ¬åƒæ•¸ï¼ˆç„¡é è¨“ç·´ï¼‰ ==============
IMG_SIZE = (192, 192)            # å…ˆç”¨ 192x192 åŠ é€Ÿï¼›ç©©å®šå†æ”¹å› 224
BATCH_SIZE = 64                  # ç›¡é‡åŠ å¤§ï¼Œèƒ½æé€Ÿä¹Ÿç©©æ¢¯åº¦
EPOCHS = 80
TRAIN_DIR = "file/kaggle_cats_vs_dogs_f/train"
VAL_DIR   = "file/kaggle_cats_vs_dogs_f/val"
MODEL_DIR = "model"; os.makedirs(MODEL_DIR, exist_ok=True)
BEST_PATH = os.path.join(MODEL_DIR, "catdog_model.h5")
IDX_PATH  = os.path.join(MODEL_DIR, "class_indices.json")
MIS_DIR   = "misclassified_vgg16_scratch"

# ============== åŠ é€Ÿé–‹é—œ ==============
# æ··åˆç²¾åº¦ï¼ˆApple/M-series or æ”¯æ´åŠç²¾åº¦çš„ GPU ä¸Šå¾ˆæœ‰æ„Ÿï¼‰
mixed_precision.set_global_policy('mixed_float16')
AUTOTUNE = tf.data.AUTOTUNE

# ============== tf.data è¼‰å…¥ ==============
def build_ds(root, training):
    ds = tf.keras.utils.image_dataset_from_directory(
        root, image_size=IMG_SIZE, batch_size=BATCH_SIZE,
        label_mode='binary', shuffle=training
    )
    # VGG çš„ 'caffe' å‰è™•ç†ï¼ˆèˆ‡ rescale=1/255 ä¸åŒï¼ï¼‰
    def _pp(x, y):
        x = tf.cast(x, tf.float32)           # åˆ° float32 å†åš caffe preprocess
        x = preprocess_input(x, mode='caffe')
        return x, y
    ds = ds.map(_pp, num_parallel_calls=AUTOTUNE)
    if training:
        ds = ds.cache().shuffle(2048)
    else:
        ds = ds.cache()
    ds = ds.prefetch(AUTOTUNE)
    return ds

train_ds = build_ds(TRAIN_DIR, training=True)
val_ds   = build_ds(VAL_DIR,   training=False)

# å„²å­˜ class_indicesï¼ˆç”± dataset è®€ä¸åˆ°ï¼Œæ‰‹å‹•å–ç›®éŒ„é †åºï¼‰
class_names = sorted(next(os.walk(TRAIN_DIR))[1])
class_indices = {name: i for i, name in enumerate(class_names)}
with open(IDX_PATH, "w") as f: json.dump(class_indices, f, indent=2, ensure_ascii=False)
print("ğŸ—‚ï¸ class_indices:", class_indices)

# ============== æ¨¡å‹ï¼ˆVGG16 å¾é›¶é–‹å§‹ï¼‰ ==============
base = VGG16(include_top=False, weights=None, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
for l in base.layers:
    l.trainable = True  # å¾é›¶è¨“ç·´ï¼Œå…¨éƒ¨å¯è¨“

x = base.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation="relu", dtype="float32")(x)  # head ç”¨ float32 æ›´ç©©
x = Dropout(0.35)(x)
out = Dense(1, activation="sigmoid", dtype="float32")(x)

model = Model(inputs=base.input, outputs=out)

# AdamW + weight decayï¼Œå­¸ç¿’ç‡é©ä¸­ï¼›é…åˆ ReduceLROnPlateau
opt = AdamW(learning_rate=1e-3, weight_decay=1e-4)
model.compile(optimizer=opt, loss="binary_crossentropy", metrics=["accuracy"])

model.summary()

# ============== Callbacks ==============
cbs = [
    ReduceLROnPlateau(monitor="val_loss", factor=0.2, patience=5, min_lr=1e-6, verbose=1),
    EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True, verbose=1),
    ModelCheckpoint(BEST_PATH, monitor="val_accuracy", save_best_only=True, verbose=1)
]

# ============== è¨“ç·´ ==============
history = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=cbs)

# å¦å­˜æœ€çµ‚
model.save(os.path.join(MODEL_DIR, "catdog_model.h5"))

# ============== è©•ä¼° & åŒ¯å‡º misclassified ==============
print("\n===== è©•ä¼°èˆ‡éŒ¯èª¤æ¨£æœ¬åŒ¯å‡º =====")
best = load_model(BEST_PATH)

# æ–¹ä¾¿ç”¨ dataset çš„æª”è·¯å¾‘ï¼šå†å»ºä¸€ä»½  batch=1 çš„é©—è­‰/è¨“ç·´ generatorï¼ˆåªåšæ¨è«–ï¼‰
from tensorflow.keras.preprocessing.image import ImageDataGenerator
eval_datagen = ImageDataGenerator(preprocessing_function=lambda x: preprocess_input(x, mode="caffe"))

def dump_mis(dataset_name, root_dir):
    gen = eval_datagen.flow_from_directory(
        root_dir, target_size=IMG_SIZE, batch_size=1,
        class_mode='binary', shuffle=False
    )
    total = len(gen.filepaths)
    probs = best.predict(gen, verbose=1)
    preds = (probs > 0.5).astype(int).flatten()
    trues = gen.classes
    paths = gen.filepaths

    # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
    dest = os.path.join(MIS_DIR, dataset_name)
    if os.path.exists(dest): shutil.rmtree(dest)
    os.makedirs(dest, exist_ok=True)
    for name in class_names: os.makedirs(os.path.join(dest, name), exist_ok=True)

    wrong = 0
    for i, (t, p) in enumerate(zip(trues, preds)):
        if int(t) != int(p):
            src = paths[i]; fname = os.path.basename(src)
            true_name = class_names[int(t)]
            shutil.copy(src, os.path.join(dest, true_name, f"wrong_pred_{fname}"))
            wrong += 1

    acc = (1 - wrong / total) * 100 if total else 0.0
    print(f"âœ… {dataset_name}: {total} å¼µï¼ŒéŒ¯ {wrong}ï¼Œæº–ç¢ºç‡ {acc:.2f}%")

for name, path in {"train": TRAIN_DIR, "val": VAL_DIR}.items():
    dump_mis(name, path)

print("\nğŸ“‚ misclassified_vgg16_scratch ç›®éŒ„ï¼š")
for root, dirs, files in os.walk(MIS_DIR):
    level = root.replace(MIS_DIR, "").count(os.sep)
    indent = "  " * level
    print(f"{indent}ğŸ“ {os.path.basename(root)}/")
    for f in files:
        print(f"{indent}  â”œâ”€ {f}")
